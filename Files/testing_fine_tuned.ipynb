{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4591a5df-afb4-4e66-a1ab-5f1d51446c2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      4\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;66;03m# Choose any! We auto support RoPE Scaling internally!\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\unsloth\\__init__.py:87\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Torch 2.5 has including_emulation\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m major_version, minor_version \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m SUPPORTS_BFLOAT16 \u001b[38;5;241m=\u001b[39m (major_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (major_torch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (minor_torch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m): \n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\torch\\cuda\\__init__.py:451\u001b[0m, in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\torch\\cuda\\__init__.py:465\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 512 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"abhi7991/promptFineTuning\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c688c49-3eac-49b2-b4c3-55b8c404b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio pytorch-cuda=<12.1/11.8> -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e1b5110-f6ec-4ab3-9948-30275dd6e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "bert = AutoTokenizer.from_pretrained(wd+\"/finetuned_phi3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a37f8b7-2b5c-498b-ac18-fcd4d00e08db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabhi7991/promptFineTuning\u001b[39m\u001b[38;5;124m\"\u001b[39m,trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, token\u001b[38;5;241m=\u001b[39mhf_token)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model = AutoModelForSequenceClassification.from_pretrained(\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     \"abhi7991/promptFineTuning\",\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     token=hf_token\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#   )\u001b[39;00m\n\u001b[0;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m      8\u001b[0m [\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mprompt\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert text to cypher query based on this schema: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# instruction\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the countries which we deliver to?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# input\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# output - leave this blank for generation!\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     14\u001b[0m ], return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextStreamer\n\u001b[0;32m     17\u001b[0m text_streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "hf_token = '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'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"abhi7991/promptFineTuning\",trust_remote_code=True, token=hf_token)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"abhi7991/promptFineTuning\",\n",
    "#     token=hf_token\n",
    "#   )\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        f\"Convert text to cypher query based on this schema: {graph.schema}\", # instruction\n",
    "        \"What are the countries which we deliver to?\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b058c663-5bd6-4656-9950-1314b36d4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from torch->bitsandbytes) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from torch->bitsandbytes) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from torch->bitsandbytes) (2024.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\finetune\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "Using cached bitsandbytes-0.43.3-py3-none-win_amd64.whl (136.5 MB)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.43.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\quantizers\\auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m peft_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabhi7991/promptFineTuning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Local model\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#peft_model_id = sft_model_path\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load Model with PEFT adapter\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpeft_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#torch_dtype=torch.float16,\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_token\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\peft\\auto.py:106\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m     )\n\u001b[1;32m--> 106\u001b[0m base_model \u001b[38;5;241m=\u001b[39m target_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_path, revision\u001b[38;5;241m=\u001b[39mbase_model_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m tokenizer_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pretrained_model_name_or_path, TOKENIZER_CONFIG_NAME)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\modeling_utils.py:3354\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3351\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3354\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   3356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3357\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3358\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "# HF model\n",
    "peft_model_id = \"abhi7991/promptFineTuning\"\n",
    "# Local model\n",
    "#peft_model_id = sft_model_path\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  #torch_dtype=torch.float16,\n",
    "  load_in_4bit=True,token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "779284cc-cedb-4213-a580-1f8fcda3d1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# from transformers import AutoModel\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model = AutoModel.from_pretrained(\"abhi7991/promptFineTuning\",token=hf_token)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoPeftModelForCausalLM\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPeftModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabhi7991/promptFineTuning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\peft\\auto.py:106\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m     )\n\u001b[1;32m--> 106\u001b[0m base_model \u001b[38;5;241m=\u001b[39m target_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_path, revision\u001b[38;5;241m=\u001b[39mbase_model_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m tokenizer_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pretrained_model_name_or_path, TOKENIZER_CONFIG_NAME)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\modeling_utils.py:3354\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3351\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3354\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[0;32m   3356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3357\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[0;32m   3358\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\finetune\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "# from transformers import AutoModel\n",
    "# model = AutoModel.from_pretrained(\"abhi7991/promptFineTuning\",token=hf_token)\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"abhi7991/promptFineTuning\",token=hf_token,load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db534ffa-2a7e-4f4a-bdbc-f4129fb91a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a89f76-010e-4113-8e3b-780817b7628b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0787ce-2c14-420c-8b91-d514d8c0e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==2.7.0\n",
      "  Downloading pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "     ---------------------------------------- 0.0/103.4 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/103.4 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/103.4 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/103.4 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/103.4 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 30.7/103.4 kB 131.3 kB/s eta 0:00:01\n",
      "     --------------------- --------------- 61.4/103.4 kB 204.8 kB/s eta 0:00:01\n",
      "     -----------------------------------  102.4/103.4 kB 311.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 103.4/103.4 kB 298.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pydantic==2.7.0) (0.7.0)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic==2.7.0)\n",
      "  Downloading pydantic_core-2.18.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pydantic==2.7.0) (4.12.2)\n",
      "Downloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "   ---------------------------------------- 0.0/407.9 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 194.6/407.9 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  399.4/407.9 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 407.9/407.9 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.9 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.6/1.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.9/1.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/1.9 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.8/1.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 5.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pydantic-core, pydantic\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.10.1\n",
      "    Uninstalling pydantic_core-2.10.1:\n",
      "      Successfully uninstalled pydantic_core-2.10.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.3\n",
      "    Uninstalling pydantic-2.7.3:\n",
      "      Successfully uninstalled pydantic-2.7.3\n",
      "Successfully installed pydantic-2.7.0 pydantic-core-2.18.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\abhis\\AppData\\Local\\Continuum\\anaconda3\\envs\\streamlit\\Lib\\site-packages\\~ydantic_core'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic_core==2.18.1 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\streamlit\\lib\\site-packages (2.18.1)\n",
      "Requirement already satisfied: typing-extensions!=4.7.0,>=4.6.0 in c:\\users\\abhis\\appdata\\local\\continuum\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pydantic_core==2.18.1) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic==2.7.0\n",
    "!pip install pydantic_core==2.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec65dd1b-bb53-40b7-9b61-8e0380fda20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from graphdatascience import GraphDataScience\n",
    "wd = os.getcwd()\n",
    "from dotenv import load_dotenv\n",
    "#import graph_build,create_plot_embeddings\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "load_dotenv()\n",
    "database = os.getenv('NEO4J_DATABASE')\n",
    "uri, user, password = os.getenv('NEO4J_URI'), os.getenv('NEO4J_USER'),'@PromptEngg'\n",
    "DEMO_URL = uri\n",
    "DATABASE = database\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=DEMO_URL,\n",
    "    database=DATABASE,\n",
    "    username=user,\n",
    "    password=password,\n",
    "    enhanced_schema=True,\n",
    "    sanitize=True,\n",
    ")\n",
    "\n",
    "myschema = graph.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a594a8-d2cb-462d-b9f7-ed903f5ed2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('FINE_TUNING_OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c6313a-ccb6-44d1-9786-2ef70c632d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error code: 404 - {'error': {'message': 'Could not find fine tune: ft:gpt-3.5-turbo-1106:personal:prompt-finetuning:9t5kHnz9', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'fine_tune_not_found'}}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "#openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# Function to retrieve fine-tuned model metrics\n",
    "def get_fine_tune_metrics(fine_tune_id):\n",
    "    try:\n",
    "        # Retrieve fine-tuned job details\n",
    "        fine_tune = client.fine_tuning.jobs.retrieve(fine_tune_id)\n",
    "        print(fine_tune)        \n",
    "        # # Print the metrics\n",
    "        # print(f\"Fine-tune ID: {fine_tune['id']}\")\n",
    "        # print(f\"Status: {fine_tune['status']}\")\n",
    "        # print(f\"Model: {fine_tune['model']}\")\n",
    "        # print(f\"Hyperparameters: {fine_tune['hyperparameters']}\")\n",
    "        # print(f\"Events: {fine_tune['events']}\")\n",
    "        # print(f\"Validation file: {fine_tune['validation_file']}\")\n",
    "        # print(f\"Training file: {fine_tune['training_file']}\")\n",
    "        # print(f\"Result files: {fine_tune['result_files']}\")\n",
    "        \n",
    "        # If you want to print specific metrics, you can parse the events\n",
    "        for event in fine_tune['events']:\n",
    "            print(event)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'YOUR_FINE_TUNE_ID' with your actual fine-tune ID\n",
    "fine_tune_id = 'ft:gpt-3.5-turbo-1106:personal:prompt-finetuning:9t5kHnz9'\n",
    "get_fine_tune_metrics(fine_tune_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c297b81-c5cd-491d-8ff3-c2b3cde6e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai.api_key = os.getenv('FINE_TUNING_OPENAI_API_KEY')\n",
    "client = OpenAI()\n",
    "vals = client.fine_tuning.jobs.list_events('ftjob-mVHT4frsPufQBkU483Hv6Sbp', limit = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b7b110-86af-4990-acc3-e4112c812647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FineTuningJobEvent(id='ftevent-cZGsZuOl1bCE3Q2EtaWpQR3Z', created_at=1722917157, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'),\n",
       " FineTuningJobEvent(id='ftevent-XZ16gMfkz8NiGYzxF5cwnVWi', created_at=1722917154, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-1106:personal:prompt-finetuning:9t5kHnz9', object='fine_tuning.job.event', data={}, type='message'),\n",
       " FineTuningJobEvent(id='ftevent-DvXCJgwj8YcblsYY8Ssxx3oN', created_at=1722917154, level='info', message='Checkpoint created at step 424 with Snapshot ID: ft:gpt-3.5-turbo-1106:personal:prompt-finetuning:9t5kHNbJ:ckpt-step-424', object='fine_tuning.job.event', data={}, type='message'),\n",
       " FineTuningJobEvent(id='ftevent-EdYKZnPGtaLn5flFeS6iEFuC', created_at=1722917154, level='info', message='Checkpoint created at step 212 with Snapshot ID: ft:gpt-3.5-turbo-1106:personal:prompt-finetuning:9t5kHuz7:ckpt-step-212', object='fine_tuning.job.event', data={}, type='message'),\n",
       " FineTuningJobEvent(id='ftevent-LPXN2APGqQDMlDoPd3ZyfT0X', created_at=1722917138, level='info', message='Step 636/636: training loss=0.44', object='fine_tuning.job.event', data={'step': 636, 'train_loss': 0.44339504837989807, 'total_steps': 636, 'train_mean_token_accuracy': 0.9464285969734192}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-pYioI4NveHHFBYVmWKAeDvL7', created_at=1722917127, level='info', message='Step 635/636: training loss=0.18', object='fine_tuning.job.event', data={'step': 635, 'train_loss': 0.1769537478685379, 'total_steps': 636, 'train_mean_token_accuracy': 0.925000011920929}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-Fp7a9RlIfiK826kMKKThptd0', created_at=1722917123, level='info', message='Step 634/636: training loss=0.04', object='fine_tuning.job.event', data={'step': 634, 'train_loss': 0.03892914205789566, 'total_steps': 636, 'train_mean_token_accuracy': 0.9729729890823364}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-vXAsGadc2eph46KEZhQU5qAd', created_at=1722917123, level='info', message='Step 633/636: training loss=0.04', object='fine_tuning.job.event', data={'step': 633, 'train_loss': 0.03704734519124031, 'total_steps': 636, 'train_mean_token_accuracy': 0.9701492786407471}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-6Sr9KXMfmBDHUl0URTQa82Fx', created_at=1722917123, level='info', message='Step 632/636: training loss=0.00', object='fine_tuning.job.event', data={'step': 632, 'train_loss': 0.00010824203491210938, 'total_steps': 636, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-Z8J2I1e9NHtZ3VPCvNFV4SDh', created_at=1722917120, level='info', message='Step 631/636: training loss=0.03', object='fine_tuning.job.event', data={'step': 631, 'train_loss': 0.02698493003845215, 'total_steps': 636, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-5ryoQ1Uu6CZjQYMIXIcJ9OpT', created_at=1722917118, level='info', message='Step 630/636: training loss=0.01', object='fine_tuning.job.event', data={'step': 630, 'train_loss': 0.012741810642182827, 'total_steps': 636, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-oQjlNrvXpW2bUDR7rdusUnJ7', created_at=1722917114, level='info', message='Step 629/636: training loss=0.15', object='fine_tuning.job.event', data={'step': 629, 'train_loss': 0.15436090528964996, 'total_steps': 636, 'train_mean_token_accuracy': 0.9411764740943909}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-VYQsydWOo8DqbawfXDYFgkvM', created_at=1722917114, level='info', message='Step 628/636: training loss=0.23', object='fine_tuning.job.event', data={'step': 628, 'train_loss': 0.2286050170660019, 'total_steps': 636, 'train_mean_token_accuracy': 0.9354838728904724}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-m9S5hUsxmq9aD05S2bwS1mVh', created_at=1722917114, level='info', message='Step 627/636: training loss=0.02', object='fine_tuning.job.event', data={'step': 627, 'train_loss': 0.018981976434588432, 'total_steps': 636, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-Wv8Ii7fScwPoR83cmfSzqPgw', created_at=1722917111, level='info', message='Step 626/636: training loss=0.04', object='fine_tuning.job.event', data={'step': 626, 'train_loss': 0.04320075362920761, 'total_steps': 636, 'train_mean_token_accuracy': 0.9818181991577148}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-OJ4iwSxxjNFXoxodwsYDCcgJ', created_at=1722917110, level='info', message='Step 625/636: training loss=0.06', object='fine_tuning.job.event', data={'step': 625, 'train_loss': 0.06351970881223679, 'total_steps': 636, 'train_mean_token_accuracy': 0.9555555582046509}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-nVr0MmkEy9qiKiNdAbXgbna6', created_at=1722917106, level='info', message='Step 624/636: training loss=0.00', object='fine_tuning.job.event', data={'step': 624, 'train_loss': 0.0015883323503658175, 'total_steps': 636, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-RaVxaFcMGT7ewu4fN1nesJAn', created_at=1722917106, level='info', message='Step 623/636: training loss=0.15', object='fine_tuning.job.event', data={'step': 623, 'train_loss': 0.1549621820449829, 'total_steps': 636, 'train_mean_token_accuracy': 0.96875}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-oZ8T3OWt9iIMYlBfowkmG0tE', created_at=1722917106, level='info', message='Step 622/636: training loss=0.00', object='fine_tuning.job.event', data={'step': 622, 'train_loss': 0.00033548142528161407, 'total_steps': 636, 'train_mean_token_accuracy': 1.0}, type='metrics'),\n",
       " FineTuningJobEvent(id='ftevent-aS7IVnlyMUYKKSvRYxzFqC7M', created_at=1722917103, level='info', message='Step 621/636: training loss=0.07', object='fine_tuning.job.event', data={'step': 621, 'train_loss': 0.07160548865795135, 'total_steps': 636, 'train_mean_token_accuracy': 0.9599999785423279}, type='metrics')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6b7c7d-b818-48e0-b0a8-125a8a43ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\"Based on the Neo4j graph schema below, write a Cypher query that would answer the user's question and provide a bried explanation to teach the user about the query: \"\n",
    "                      \"\\n{schema}\").format(schema=myschema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a95897a8-f409-47f5-b58b-7f866fb95927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='MATCH (p:Product)-[:WEIGHT]->(c:Country)\\nRETURN DISTINCT c.name\\nIn this query, we are matching the pattern of a Product node connected to a Country node via the WEIGHT relationship. We then return the distinct names of the countries to which we supply products. The DISTINCT keyword ensures that each country name is only returned once, even if there are multiple products supplied to the same country.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-1106:personal:prompt-finetuning:9t5kHnz9\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"What are the countries which we supply to?\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9cf9f55-afe0-4543-9538-b72851ea1687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (p:Product)-[:WEIGHT]->(c:Country)\n",
      "RETURN DISTINCT c.name\n",
      "In this query, we are matching the pattern of a Product node connected to a Country node via the WEIGHT relationship. We then return the distinct names of the countries to which we supply products. The DISTINCT keyword ensures that each country name is only returned once, even if there are multiple products supplied to the same country.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4a443-325d-41b6-a50f-2351b6b31f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d363c7f-8e4c-4fd5-b16f-e7eb77fe061a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
