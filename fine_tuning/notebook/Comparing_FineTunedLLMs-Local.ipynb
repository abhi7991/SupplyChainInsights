{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0d6a49-bc5f-4953-aac7-ea389c3bcaef",
   "metadata": {},
   "source": [
    "#!pydantic_core==2.10.1\n",
    "# Comparing Fine Tuned Models using Ollama - Locally\n",
    "\n",
    "## I used 3 fine tuned models for this analayis. The models have also been deployed on HuggingFace\n",
    "\n",
    "- ### [Phi3](https://huggingface.co/abhi7991/promptFineTuning)\n",
    "- ### [Llama3 - 8b](https://huggingface.co/abhi7991/promptfinetuning-llama3)\n",
    "- ### [Llama3 - 8b (Developed by Neo4j)](https://huggingface.co/collections/tomasonjo/llama3-text2cypher-demo-6647a9eae51e5310c9cfddcf)\n",
    "\n",
    "I fine tuned Phi3 and Llama3b to convert natural language queries to cypher queries which can be used to tap into Neo4j knowledge graphs.\n",
    "When combined with Neo4jGraph it can execute the Cypher queries and obtain results in natural language. \n",
    "\n",
    "**Refer to the previous notebooks to see how the models were finetuned and to provide you with a deeper insight on how to go about it as well as challenges faced**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005af107-1a1f-48cc-bda0-e1df76d45d16",
   "metadata": {},
   "source": [
    "# My Fine tuned model\n",
    "\n",
    "This fine tuned model consists of Llama3 trained on my custom dataset to provide it with much more accurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7734a6f1-4f0a-440a-9d64-9a1a56a2849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (p:Product)-[w:WEIGHT]-(c:Country)\n",
      "RETURN c.name AS Country, COUNT(w) AS ProductCount\n",
      "ORDER BY ProductCount DESC;\n",
      "Reponse Time for Question 1:  380.12241530418396\n",
      "MATCH (p:Product)-[:BRAND]->(b:Brand)\n",
      "RETURN b.name, count(b.name) AS BrandCount\n",
      "GROUP BY b.name\n",
      "ORDER BY BrandCount DESC;\n",
      "Reponse Time for Question 2:  304.29602813720703\n",
      "MATCH (o:Office)-[:MANAGED_BY]->(p:Product)\n",
      "RETURN o.name, count(p) AS product_count\n",
      "ORDER BY product_count DESC;\n",
      "Reponse Time for Question 3:  262.7514851093292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "database = os.getenv('NEO4J_DATABASE')\n",
    "uri, user, password = os.getenv('NEO4J_URI'), os.getenv('NEO4J_USER'),'@PromptEngg'\n",
    "DEMO_URL = uri\n",
    "DATABASE = database\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=DEMO_URL,\n",
    "    database=DATABASE,\n",
    "    username=user,\n",
    "    password=password,\n",
    "    enhanced_schema=True,\n",
    "    sanitize=True,\n",
    ")\n",
    "llm = ChatOllama(model=\"text2cypher-llama3\", request_timeout=60.0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given an input question, convert it to a Cypher query. No pre-amble.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Based on the Neo4j graph schema below, write a Cypher query that would answer the user's question: \"\n",
    "                \"\\n{schema} \\nQuestion: {question} \\nCypher query:\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "questions = [\"What are the various countries products get shipped to?\", \n",
    "             \"How many different Brands are there in the supply chain\",\n",
    "            \"Give me the count of products handled by each office?\"]\n",
    "i = 0\n",
    "for q in questions:\n",
    "\n",
    "    t0 = time.time()    \n",
    "    question = q\n",
    "    response = chain.invoke({\"question\": question, \"schema\": graph.schema})\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    i+=1\n",
    "    print(\"Reponse Time for Question \"+str(i)+\": \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bb3d0-44ce-4a8e-a7eb-54cc692a488f",
   "metadata": {},
   "source": [
    "# Neo4j Data Scientist Fine tuned model\n",
    "\n",
    "The Data Scientists at Neo4j have trained a model which is bigger on a larger dataset andd the results are shown as below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f7245b-9b28-44bd-8ca6-ab65b913958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "from graphdatascience import GraphDataScience\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "database = os.getenv('NEO4J_DATABASE')\n",
    "uri, user, password = os.getenv('NEO4J_URI'), os.getenv('NEO4J_USER'),'@PromptEngg'\n",
    "DEMO_URL = uri\n",
    "DATABASE = database\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=DEMO_URL,\n",
    "    database=DATABASE,\n",
    "    username=user,\n",
    "    password=password,\n",
    "    enhanced_schema=True,\n",
    "    sanitize=True,\n",
    ")\n",
    "llm = ChatOllama(model=\"tomasonjo/llama3-text2cypher-demo\", request_timeout=60.0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given an input question, convert it to a Cypher query. No pre-amble.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Based on the Neo4j graph schema below, write a Cypher query that would answer the user's question: \"\n",
    "                \"\\n{schema} \\nQuestion: {question} \\nCypher query:\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "questions = [\"What are the various countries products get shipped to?\", \n",
    "             \"How many different Brands are there in the supply chain\",\n",
    "            \"Give me the count of products handled by each office?\"]\n",
    "i = 0\n",
    "for q in questions:\n",
    "\n",
    "    t0 = time.time()    \n",
    "    question = q\n",
    "    response = chain.invoke({\"question\": question, \"schema\": graph.schema})\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1-t0\n",
    "    i+=1\n",
    "    print(\"Reponse Time for Question \"+str(i)+\": \", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278ca64-a9af-48b5-8440-8a81ce733f13",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "After viewing our models performance and latency we can see that while the results were right, the time taken to generate the results was long.\n",
    "\n",
    "it's essential to understand the factors that influence Ollama's performance:\n",
    "\n",
    "- **Hardware capabilities (CPU, RAM, GPU)**\n",
    "- **Model size and complexity**\n",
    "- **Quantization level**\n",
    "- **Context window size**\n",
    "- **System configuration and settings**\n",
    "\n",
    "## Key Takeaways \n",
    "- The model size is another important parameter to take into consideration, I used Llama3 which is quite large and could have contributed to the latency.\n",
    "  \n",
    "- In this case the configurations of my local environment were as follows - The right GPU was not configured and would have sped up the process. \n",
    "\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 560.76                 Driver Version: 560.76         CUDA Version: 12.6     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
    "| N/A   59C    P0             15W /   50W |    3578MiB /   4096MiB |      0%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
